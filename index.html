<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Reconnaissance Crawler - Documentation</title>
    <style>
        :root {
            --primary-color: #2c3e50;
            --accent-color: #2980b9;
            --bg-color: #f8f9fa;
            --text-color: #333;
            --code-bg: #eaeaea;
            --warning-bg: #fff3cd;
            --warning-border: #ffeeba;
            --warning-text: #856404;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 40px;
            background: white;
            box-shadow: 0 4px 15px rgba(0,0,0,0.05);
            border-radius: 8px;
        }

        h1 {
            font-size: 2.5em;
            color: var(--primary-color);
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
            margin-bottom: 30px;
        }

        h2 {
            font-size: 1.8em;
            color: var(--primary-color);
            margin-top: 40px;
            border-left: 5px solid var(--accent-color);
            padding-left: 15px;
        }

        h3 {
            font-size: 1.3em;
            color: var(--accent-color);
            margin-top: 25px;
        }

        p {
            margin-bottom: 15px;
        }

        ul {
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Highlighting Logic */
        strong {
            color: var(--primary-color);
            font-weight: 700;
        }

        .highlight-keyword {
            background-color: #e8f4fc;
            padding: 0 4px;
            border-radius: 4px;
            color: var(--accent-color);
            font-weight: bold;
        }

        /* Special Components */
        .warning-box {
            background-color: var(--warning-bg);
            border: 1px solid var(--warning-border);
            color: var(--warning-text);
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            border-left: 5px solid #ffc107;
        }

        .code-block {
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Consolas', 'Monaco', monospace;
            overflow-x: auto;
            margin: 15px 0;
        }

        .architecture-tree {
            background-color: #f1f3f5;
            padding: 20px;
            border-radius: 5px;
            font-family: monospace;
            white-space: pre;
            overflow-x: auto;
            border: 1px solid #ddd;
            color: #444;
        }

        .status-bar {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            text-align: center;
            font-style: italic;
            color: #666;
            font-weight: bold;
        }

        @media (max-width: 768px) {
            .container {
                margin: 0;
                padding: 20px;
                border-radius: 0;
            }
        }
    </style>
</head>
<body>

<div class="container">

    <h1>Web Reconnaissance Crawler</h1>

    <h2>1. Introduction</h2>
    <p>The <strong>Web Reconnaissance Crawler</strong> is an <span class="highlight-keyword">ethical, non‚Äëintrusive</span> security reconnaissance tool built to assist authorized penetration testers and security analysts during the early stages of web application security assessments.</p>
    
    <p>The tool <span class="highlight-keyword">automates</span> the information‚Äëgathering (recon) phase by crawling a target website and extracting structural and configuration‚Äëlevel details that help testers plan manual vulnerability testing more effectively.</p>

    <div class="warning-box">
        <strong>‚ö†Ô∏è Legal Notice:</strong> This tool must be used <strong>only</strong> on systems you own or have <strong>explicit permission</strong> to test.
    </div>

    <h3>1.1. üìç Key Responsibilities & Features</h3>
    <ul>
        <li><strong>Implemented</strong> same-domain web crawling with <strong>depth control</strong> and <strong>rate limiting</strong> to prevent excessive requests.</li>
        <li><strong>Discovered</strong> and classified web endpoints, <strong>URL parameters</strong>, and <strong>HTML forms</strong> for attack surface analysis.</li>
        <li><strong>Performed</strong> passive security checks such as <strong>missing security headers</strong> and <strong>HTTP to HTTPS</strong> enforcement.</li>
        <li><strong>Generated</strong> structured reconnaissance reports in <strong>JSON</strong> and <strong>CSV</strong> formats for manual security analysis.</li>
    </ul>

    <h3>üìç Impact / Learning</h3>
    <ul>
        <li><strong>Reduced</strong> manual reconnaissance effort by automating initial information gathering.</li>
        <li><strong>Gained</strong> hands-on experience in <strong>web security fundamentals</strong> and <strong>ethical penetration testing</strong> practices.</li>
    </ul>

    <hr>

    <h2>2. Problem Statement</h2>
    <p>Manual reconnaissance during penetration testing is:</p>
    <ul>
        <li><strong>Time‚Äëconsuming</strong></li>
        <li><strong>Repetitive</strong></li>
        <li><strong>Prone to human error</strong></li>
    </ul>
    <p>Security testers need a safe, automated way to map <strong>application endpoints</strong>, <strong>user‚Äëcontrolled inputs</strong>, <strong>forms</strong>, and <strong>basic security misconfigurations</strong> without performing active exploitation.</p>

    <h2>3. Objectives</h2>
    <ul>
        <li><strong>Automate</strong> reconnaissance in a controlled and ethical manner.</li>
        <li><strong>Identify</strong> application attack surface.</li>
        <li><strong>Reduce</strong> manual effort before exploitation.</li>
        <li><strong>Maintain</strong> strict non‚Äëexploitative behavior.</li>
    </ul>

    <hr>

    <h2>4. Scope of the Project</h2>

    <h3>‚úÖ Included Scope</h3>
    <ul>
        <li><strong>Same‚Äëdomain</strong> web crawling.</li>
        <li><strong>Endpoint</strong> and parameter discovery.</li>
        <li><strong>Form</strong> and input field mapping.</li>
        <li><strong>Passive security</strong> configuration checks.</li>
        <li><strong>Technology fingerprinting</strong> (basic).</li>
        <li><strong>Report</strong> generation.</li>
    </ul>

    <h3>‚ùå Out of Scope (Intentionally)</h3>
    <ul>
        <li><strong>Vulnerability exploitation</strong>.</li>
        <li><strong>Payload injection</strong> (SQLi, XSS, etc.).</li>
        <li><strong>Authentication bypass</strong> attempts.</li>
        <li><strong>Brute‚Äëforce</strong> or fuzzing attacks.</li>
    </ul>

    <hr>

    <h2>5. System Overview</h2>
    <p>The crawler starts from a given <strong>Target URL</strong> and systematically explores all reachable internal pages within a defined <strong>depth limit</strong>. It analyzes HTTP responses and HTML content to extract meaningful reconnaissance data.</p>

    <h2>6. Architecture Description</h2>
    <p>The following tree describes the high-level data flow:</p>

    <div class="architecture-tree">
User Input (Target URL)
   ‚Üì
Crawler Controller
   ‚îú‚îÄ‚îÄ URL Queue Manager
   ‚îú‚îÄ‚îÄ HTTP Request Engine
   ‚îÇ      ‚îú‚îÄ‚îÄ Rate Limiter
   ‚îÇ      ‚îî‚îÄ‚îÄ Session Handler
   ‚îú‚îÄ‚îÄ Response Analyzer
   ‚îÇ      ‚îú‚îÄ‚îÄ Link Extractor
   ‚îÇ      ‚îú‚îÄ‚îÄ Form Parser
   ‚îÇ      ‚îî‚îÄ‚îÄ Header Analyzer
   ‚îú‚îÄ‚îÄ Passive Security Check Module
   ‚îî‚îÄ‚îÄ Report Generator
    </div>

    <h2>7. Module Description</h2>

    <h3>7.1 Crawler Controller</h3>
    <ul>
        <li>Manages overall <strong>crawl flow</strong>.</li>
        <li>Enforces <strong>depth</strong> and <strong>scope</strong> limits.</li>
    </ul>

    <h3>7.2 URL Queue Manager</h3>
    <ul>
        <li>Maintains the <strong>crawl queue</strong>.</li>
        <li>Prevents <strong>duplicate crawling</strong>.</li>
        <li>Avoids <strong>infinite loops</strong>.</li>
    </ul>

    <h3>7.3 HTTP Request Engine</h3>
    <ul>
        <li>Sends <strong>HTTP GET</strong> requests.
        <li>Applies request delays (<strong>rate limiting</strong>).</li>
        <li>Manages <strong>headers</strong> and <strong>cookies</strong>.</li>
    </ul>

    <h3>7.4 Response Analyzer</h3>
    <ul>
        <li><strong>Parses</strong> HTML responses.</li>
        <li>Extracts <strong>links</strong>, <strong>parameters</strong>, and <strong>forms</strong>.</li>
        <li>Analyzes response <strong>headers</strong>.</li>
    </ul>

    <h3>7.5 Passive Security Check Module</h3>
    <ul>
        <li>Detects <strong>missing security headers</strong>.</li>
        <li>Identifies <strong>HTTP to HTTPS</strong> issues.</li>
        <li>Flags potential <strong>directory exposure</strong>.</li>
    </ul>

    <h3>7.6 Report Generator</h3>
    <ul>
        <li>Generates <strong>JSON</strong> and <strong>CSV</strong> outputs.</li>
        <li><strong>Aggregates</strong> all discovered data.</li>
    </ul>

    <hr>

    <h2>8. Features Description</h2>

    <h3>Smart Crawling</h3>
    <ul>
        <li><strong>Same‚Äëdomain restriction</strong>: Ensures the crawler does not drift to external sites.</li>
        <li><strong>Crawl depth control</strong>: Limits how deep the crawler goes into the directory structure.</li>
        <li><strong>Request throttling</strong>: Prevents DoS conditions on the target.</li>
    </ul>

    <h3>Endpoint Discovery</h3>
    <ul>
        <li>Identifies <strong>static</strong> and <strong>dynamic</strong> URLs.</li>
        <li>Extracts <strong>query parameters</strong>.</li>
        <li>Records <strong>HTTP status codes</strong>.</li>
    </ul>

    <h3>Form Analysis</h3>
    <ul>
        <li>Detects <strong>HTML forms</strong>.</li>
        <li>Extracts <strong>input field names</strong> and <strong>types</strong>.</li>
        <li>Identifies form <strong>actions</strong> and <strong>methods</strong> (GET/POST).</li>
    </ul>

    <h3>Passive Security Checks</h3>
    <ul>
        <li>Checks for <strong>Missing Security Headers</strong> (e.g., X-Frame-Options, HSTS).</li>
        <li>Flags <strong>Insecure HTTP</strong> usage.</li>
        <li>Identifies <strong>Input reflection</strong> indicators (non‚Äëexploitative).</li>
    </ul>

    <hr>

    <h2>9. Technology Stack</h2>
    <ul>
        <li><strong>Programming Language:</strong> Python 3</li>
        <li><strong>Libraries Used:</strong>
            <ul>
                <li><code>requests</code> (HTTP handling)</li>
                <li><code>beautifulsoup4</code> (HTML parsing)</li>
                <li><code>urllib</code> (URL manipulation)</li>
            </ul>
        </li>
    </ul>

    <hr>

    <h2>10. Installation & Usage</h2>

    <h3>Installation</h3>
    <div class="code-block">
pip install requests beautifulsoup4
    </div>

    <h3>Usage</h3>
    <ol>
        <li>Set the <strong>target URL</strong> in the script.</li>
        <li>Run the crawler:</li>
    </ol>
    <div class="code-block">
python web_recon_crawler_main.py
    </div>

    <h3>Output Files</h3>
    <ul>
        <li><code>report.json</code> ‚Äì Detailed structured report.</li>
        <li><code>endpoints.csv</code> ‚Äì List of discovered endpoints.</li>
    </ul>

    <hr>

    <h2>11. Ethical Considerations</h2>
    <ul>
        <li><strong>No active exploitation</strong> performed.</li>
        <li><strong>No payload injection</strong> used.</li>
        <li><strong>Scope‚Äërestricted</strong> crawling enforced.</li>
        <li><strong>Explicit authorization</strong> required before use.</li>
    </ul>

    <h2>12. Limitations</h2>
    <ul>
        <li>Limited <strong>JavaScript rendering</strong> (cannot crawl Single Page Applications easily).</li>
        <li>No <strong>authenticated crawling</strong> by default.</li>
        <li>Findings are <strong>heuristic</strong>, not definitive proof of vulnerability.</li>
    </ul>

    <h2>13. Future Enhancements</h2>
    <ul>
        <li><strong>Authenticated session</strong> crawling.</li>
        <li><strong>Headless browser</strong> integration (e.g., Selenium/Playwright) for JS support.</li>
        <li><strong>Visual dashboard</strong> for reporting.</li>
        <li><strong>Risk‚Äëbased</strong> endpoint scoring.</li>
    </ul>

    <hr>

    <h2>14. Conclusion</h2>
    <p>The <strong>Web Reconnaissance Crawler</strong> provides a practical, ethical, and <strong>interview‚Äëready</strong> demonstration of <strong>security‚Äëoriented thinking</strong>. It reflects a strong understanding of the <strong>reconnaissance phase</strong> of penetration testing while maintaining professional and legal boundaries.</p>

    <div class="status-bar">
        Project Status: Completed ‚Ä¢ Interview‚ÄëReady ‚Ä¢ Ethical
    </div>
         
        <div>
            Yes, we can significantly improve this. To take this from a "student project" level to a "professional security tool" level, we need to focus on three areas:
 * Technical Sophistication: Moving from simple loops to concurrency (speed) and proper input handling.
 * Depth of Recon: extracting more specific, actionable security data.
 * Usability: Making it a proper Command Line Interface (CLI) tool rather than "edit the script."
Here are the specific improvements and a rewritten, high-impact version of the documentation reflecting these upgrades.
üöÄ Proposed Improvements
1. Architecture Upgrade (The "Pro" Touch)
 * Asynchronous Crawling: Instead of requests (which blocks), mention the use of aiohttp and asyncio. This makes the crawler 10x faster.
 * Producer-Consumer Pattern: Describe the URL Queue as a standard Queue data structure where "workers" pull URLs to process in parallel.
 * Robots.txt Compliance: To be truly "ethical," the crawler must parse and respect robots.txt.
2. Feature Expansions
 * Sensitive File Discovery: Look for exposed .git, .env, or backup.zip files (common recon tasks).
 * Comment Analysis: Extract HTML comments (``) which often leak info.
 * Tech Stack Detection: Analyze Server headers and Wappalyzer-style patterns (e.g., "WordPress", "React").
3. Usability Upgrades
 * CLI Arguments: Use argparse or click. Users should run python crawler.py --url http://target.com --depth 3 instead of editing code.
 * Live Logging: Implement a proper logging module (logging) instead of print().
üìù The "Next-Level" Documentation
Below is the upgraded documentation. I have rewritten the Architecture, Features, and Tech Stack to reflect a more advanced tool.
üõ°Ô∏è WebSec Recon: Automated Security Discovery Engine
1. Executive Summary
WebSec Recon is a high-performance, asynchronous web crawler designed for passive security reconnaissance. It automates the mapping of an application's attack surface by spidering the target domain, indexing endpoints, and performing non-intrusive security heuristics.
Designed for Red Teams and Bug Bounty hunters, it adheres to strict ethical guidelines by respecting robots.txt policies and performing zero-impact analysis (no payloads sent).
> Context: This tool bridges the gap between manual browsing and active vulnerability scanning, providing a comprehensive "blueprint" of the target before testing begins.
> 
2. Architecture & Design
The system utilizes a Producer-Consumer architecture powered by Python's asyncio event loop to handle concurrent requests efficiently.
2.1 Component Flow
 * Orchestrator: Initializes the thread pool and loads configurations.
 * Robots.txt Parser: Fetches the allow/disallow rules immediately.
 * The Frontier (Queue): A priority queue managing pending URLs.
 * Async Workers: Multiple workers fetch URLs concurrently using aiohttp.
 * Extraction Engine:
   * DOM Parser: Extracts href, src, and forms using BeautifulSoup.
   * Comment Scraper: Hunts for "TODO", "FIXME", or API keys in HTML comments.
 * Auditor: Checks headers (CORS, CSP, HSTS) against best practices.
 * Data Persistence: Streams results to JSON/CSV in real-time.
3. Advanced Feature Set
üï∑Ô∏è Intelligent Crawling
 * Asynchronous Core: Capable of handling 50+ concurrent requests.
 * Scope Enforcement: Strict Regex-based filtering to keep the crawler within the authorized domain.
 * Smart Rate Limiting: Jitter-based delays to avoid WAF detection and server overload.
 * Robots.txt Adherence: Automatically parses and respects exclusion rules.
üîç Attack Surface Mapping
 * Endpoint Inventory: Catalogs all static assets (JS, CSS, Images) and dynamic pages.
 * Parameter Extraction: Parses query strings (e.g., ?id=1&action=view) to identify potential SQLi/XSS entry points.
 * Form Fingerprinting: Maps all <form> tags, identifying input names, methods (POST/GET), and hidden fields.
üõ°Ô∏è Passive Security Auditing
 * Header Analysis: Flags missing security headers (X-Frame-Options, Content-Security-Policy, Strict-Transport-Security).
 * Sensitive File Probing: Checks for common accidental exposures (e.g., .git/config, .env, .ds_store).
 * Information Leakage: Detects server version banners and interesting HTML comments.
 * Cookie Inspection: Flags cookies missing Secure or HttpOnly attributes.
4. Technical Specifications
| Component | Technology | Reasoning |
|---|---|---|
| Language | Python 3.9+ | Robust ecosystem for security tooling. |
| Networking | aiohttp | Non-blocking I/O for high-speed crawling. |
| Parsing | BeautifulSoup4 + lxml | Fast, lenient HTML parsing. |
| CLI | argparse | Unix-style command line argument parsing. |
| Output | JSON / CSV | Universal formats for pipe-lining into other tools. |
5. Installation & Usage
Setup
git clone https://github.com/yourname/websec-recon.git
pip install -r requirements.txt

Usage Examples
Basic Crawl:
python3 main.py -u https://example.com

Advanced Crawl (Depth 3, 10 Threads, Save to JSON):
python3 main.py -u https://example.com -d 3 -t 10 -o report.json

6. Sample Report Output (JSON)
{
  "target": "https://example.com",
  "scan_duration": "14.2s",
  "vulnerabilities": [
    {
      "type": "Missing Header",
      "severity": "Low",
      "detail": "X-Frame-Options header not set (Clickjacking risk)."
    },
    {
      "type": "Sensitive File",
      "severity": "High",
      "detail": "Accessible .git directory found at https://example.com/.git/"
    }
  ],
  "attack_surface": {
    "forms": 3,
    "input_params": ["search", "id", "user_token"]
  }
}

7. Future Roadmap
 * Headless Browser: Integration with Playwright to crawl Single Page Applications (SPAs) like React/Vue.
 * Screenshotting: Auto-capture screenshots of discovered login portals.
 * Graph Visualization: Generate a force-directed graph of the site structure.
üí° Why this is better:
 * Keywords: It uses industry terms like "Attack Surface," "Asynchronous," "Orchestrator," and "Heuristics."
 * Functionality: It sounds like a tool a professional would actually use, not just a homework assignment.
 * Depth: It explicitly mentions checking for .git files and cookie flags, which shows you understand what to look for, not just how to write Python.

        </div>
</div>

</body>
</html>
